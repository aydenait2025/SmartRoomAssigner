# 📊 Data Management & Processing

This directory serves as the **centralized data hub** for SmartRoomAssigner, containing all CSV datasets, data transformation artifacts, and data processing pipelines. All data operations follow a strict raw→processed workflow to maintain data integrity.

## 📁 Directory Structure

```
data/
├── raw/                          # 📥 Source data (immutable originals)
│   ├── buildings_with_headers.csv    # Building codes with headers
│   ├── buildings_space_separated.csv # Alternative CSV formats
│   └── [future raw data sources]
└── processed/                    # 📤 Clean, processed datasets
    ├── buildings.csv                 # Primary building codes (processed)
    ├── rooms.csv                     # Room capacities (generated)
    └── [other processed outputs]
```

## 🔄 Data Pipeline Architecture

### **Input → Processing → Output**
```
Raw Data Sources (Immutable)
        ↓
Utility Scripts (utils/*.py)
        ↓
Processed Datasets (Ready for Import)
        ↓
Database Integration (PostgreSQL)
```

## 📋 Data File Descriptions

### **🏢 Building Data Files**

#### **`raw/buildings_with_headers.csv`**
- **Purpose**: Source building data with column headers
- **Format**: `BuildingCode,BuildingName`
- **Origin**: External data source or manual compilation
- **Usage**: Reference dataset for building name lookups

#### **`raw/buildings_space_separated.csv`**
- **Purpose**: Alternative building data format
- **Format**: Space-separated values (legacy format)
- **Origin**: Converted from other sources
- **Usage**: Backup/alternative building data

#### **`processed/buildings.csv`** ⭐ **PRIMARY**
- **Purpose**: Core building codes for processing
- **Format**: `CODE,Building Name` (no headers, comma-separated)
- **Origin**: Generated by `utils/scrape_buildings.py`
- **Usage**: Input for room scraping, database imports
- **Critical**: All room scraping depends on this file

#### **`processed/rooms.csv`** ⭐ **GENERATED**
- **Purpose**: Complete room capacity database
- **Format**: `Building,Room,Room Capacity,Testing Capacity` (with headers)
- **Origin**: Generated by `utils/scrape_rooms.py`
- **Usage**: Database seeding, capacity planning
- **Dependencies**: `processed/buildings.csv`

## 🚀 Data Generation Workflow

### **Step 1: Collect Buildings**
```bash
cd utils
python scrape_buildings.py
# Output: ../../data/processed/buildings.csv
```

### **Step 2: Collect Room Capacities**
```bash
cd utils
python scrape_rooms.py
# Input:  ../../data/processed/buildings.csv
# Output: ../../data/processed/rooms.csv
```

### **Step 3: Validate & Import**
```bash
cd utils
python test_import.py          # Validate CSV formats
python test_comprehensive.py   # End-to-end pipeline test
```

## 📊 Data Quality Standards

### **Raw Data (Immutable)**
- ✅ Changes **NEVER** made to raw files
- ✅ Original timestamps and formats preserved
- ✅ Used only as reference/input sources
- ✅ Version controlled for audit trails

### **Processed Data (Generated)**
- ✅ Automatically regenerated from raw data
- ✅ Standard formats and naming conventions
- ✅ Validation checks and error reporting
- ✅ Clean, consistent data structures

## 🔍 Data Validation Rules

### **Building Data Validation**
```python
# Required checks for buildings.csv
- No duplicate building codes
- Valid UofT building code format (^[A-Z0-9]{2,4}$)
- No empty building names
- Consistent code-to-name mappings
```bash
# Manual validation
cd data/processed
awk -F ',' 'length($1) < 2 || length($1) > 4 {print "Invalid code:", $1}' buildings.csv
```

### **Room Data Validation**
```python
# Required checks for rooms.csv
- Building codes exist in buildings.csv
- Room capacities are positive integers
- No duplicate room entries within buildings
- Testing capacity >= regular capacity
```bash
# Manual validation
cd data/processed
# Check for missing capacities
grep -c '^[^,]*,"",.*' rooms.csv
```

## 🔄 Data Maintenance Procedures

### **Weekly Updates**
```bash
# Regenerate all data (typically run weekly)
cd utils
python scrape_buildings.py
python scrape_rooms.py

# Validate results
python test_import.py
```

### **Monthly Quality Assurance**
- Review generated data for accuracy
- Compare with official UofT sources
- Remove obsolete building codes
- Update room capacity information

### **Annual Refresh**
- Complete system rebuild from scratch
- Verify all building/room data
- Update scraping scripts for LMS changes
- Document any schema changes

## 🚨 Data Integrity Guidelines

### **🔒 Raw Data Protection**
- **NEVER modify** files in `raw/` directory
- Create new versions if source data changes
- Use raw data only as immutable inputs

### **♻️ Processed Data Management**
- Processed files are **transient** and **regenerable**
- Can be safely deleted and recreated
- Version control optional (often large datasets)
- Backup critical files only when needed

### **🔄 Pipeline Consistency**
- Always run full pipeline (buildings → rooms)
- Never skip validation steps
- Document any manual data corrections
- Track data quality metrics

## 🐛 Troubleshooting Data Issues

### **"File not found: buildings.csv"**
```bash
# Solution: Regenerate building data
cd utils && python scrape_buildings.py
ls -la ../data/processed/buildings.csv  # Verify creation
```

### **"No buildings found"**
- Check UofT LMS system availability
- Verify network connectivity
- Use VPN if required
- Contact IT if system is down

### **Inconsistent room counts**
```bash
# Compare building counts
wc -l data/processed/buildings.csv
wc -l data/processed/rooms.csv  # Should be much higher (rooms > buildings)
```

### **Data format errors**
```bash
# Validate CSV structure
cd data/processed
head -5 buildings.csv | cat -A  # Check for hidden characters
tail -5 rooms.csv | cat -A     # Verify consistent formatting
```

## 📈 Data Analytics & Reporting

### **Available Metrics**
- Building counts by campus
- Room capacity distribution
- Utilization patterns (future)
- Data freshness indicators
- Quality assurance scores

### **Reporting Queries**
```sql
-- Building summary
SELECT campus, COUNT(*) as buildings
FROM buildings GROUP BY campus;

-- Capacity analysis
SELECT
    building,
    COUNT(*) as rooms,
    SUM(capacity) as total_capacity,
    AVG(capacity) as avg_room_size
FROM rooms GROUP BY building;
```

## 🎯 Best Practices

### **🏗️ Data Pipeline Development**
- Maintain clear input→output relationships
- Document all data transformations
- Include error handling and logging
- Test with both small and large datasets

### **🔐 Security Considerations**
- No sensitive PII stored in data files
- CSV files contain only public building/room information
- Access controls based on file permissions
- Regular security audits of data pipelines

### **📏 Scaling Considerations**
- For large datasets (>100MB), consider binary formats
- Implement incremental updates rather than full rebuilds
- Monitor memory usage in data processing scripts
- Consider parallel processing for campus-wide data collection

## 📞 Support & Data Issues

**Data Pipeline Issues:**
1. Run `test_import.py` for automated validation
2. Check UofT LMS system status
3. Verify network connectivity and VPN requirements
4. Review execution logs in utils/ directory

**Data Quality Concerns:**
1. Compare with official UofT publications
2. Validate against multiple data sources
3. Document discrepancies for IT review
4. Consider manual corrections only as last resort

**Maintainer**: SmartRoomAssigner Data Operations
**Contact**: Development team for data pipeline issues
